{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6fe8bbf",
   "metadata": {},
   "source": [
    "# Run it in Google colab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d782943",
   "metadata": {},
   "source": [
    "# 403. Training Process in LSTM RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21681e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f32633f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data (2 sentences, 0=negative, 1=positive)\n",
    "texts = [\"I love this movie\", \"I hate this film\"]\n",
    "labels = [1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3594ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Tokenization\n",
    "tokenizer = Tokenizer(num_words=1000)  # Max vocabulary size\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f23cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Padding to ensure equal length\n",
    "max_length = max(len(seq) for seq in sequences)\n",
    "X = pad_sequences(sequences, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8874679d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Labels as NumPy array\n",
    "y = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd87a2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Build LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=1000, output_dim=64, input_length=max_length))\n",
    "model.add(LSTM(units=32)) # units=32: This means the hidden state vector (ht) and cell state (Ct) will each have 32 dimensions. So, each time step, the LSTM outputs a 32-dimensional vector.\n",
    "model.add(Dense(1, activation='sigmoid'))  # Binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e51af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Compile and train\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c377fce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Train\n",
    "model.fit(X, y, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea42e837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Prediction example\n",
    "test_sentence = [\"I enjoy this film\"]\n",
    "test_seq = tokenizer.texts_to_sequences(test_sentence)\n",
    "test_pad = pad_sequences(test_seq, maxlen=max_length)\n",
    "prediction = model.predict(test_pad)\n",
    "\n",
    "print(\"Prediction:\", prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
